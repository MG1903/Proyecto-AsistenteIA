import os
from typing import Optional, List, Mapping, Any, Tuple
from dotenv import load_dotenv
from openai import OpenAI

from langchain_chroma import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM

# ConfiguraciÃ³n del entorno y constantes
load_dotenv()

CHROMA_DB_PATH = "./chroma_db"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
LLM_MODEL_NAME = "deepseek-chat"
API_URL = "https://api.deepseek.com"

# Factor de calibraciÃ³n para normalizar distancias euclidianas a porcentajes humanos
# MiniLM suele generar distancias L2 entre 0.8 y 1.2 para textos relacionados.
SIMILARITY_SCALE_FACTOR = 2.0 

SYSTEM_PROMPT_TEMPLATE = """
Eres un asistente virtual experto en relojerÃ­a.
Saluda siempre con: "Â¡Hola! ðŸ˜Š Â¿En quÃ© puedo ayudarte hoy?" (sin decir que eres experto).
Tu objetivo es responder SIEMPRE en espaÃ±ol, de forma amable, Ãºtil y natural.
Usa el contexto proporcionado si es relevante. Si no tienes informaciÃ³n exacta en el contexto, 
ofrece ayuda general relacionada con relojerÃ­a, horarios o servicios, pero no inventes datos especÃ­ficos.

Contexto disponible:
{context}

Pregunta del cliente:
{question}

Responde de forma clara, breve y con tono conversacional.
"""

# InicializaciÃ³n de Cliente API
deepseek_client = OpenAI(
    api_key=os.environ.get("DEEPSEEK_API_KEY"),
    base_url=API_URL
)

class DeepSeekLLM(LLM):
    """
    Wrapper personalizado para integrar la API de DeepSeek con LangChain via OpenAI SDK.
    """
    
    @property
    def _llm_type(self) -> str:
        return "deepseek"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model": LLM_MODEL_NAME}

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        # La llamada es sÃ­ncrona por diseÃ±o de LangChain BaseLLM
        response = deepseek_client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": "ActÃºa como un asistente Ãºtil y amable de relojerÃ­a."},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content

# InicializaciÃ³n de Componentes RAG
# Se instancian a nivel de mÃ³dulo para actuar como Singleton en la ejecuciÃ³n de Django
embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)

vectorstore = Chroma(
    persist_directory=CHROMA_DB_PATH,
    embedding_function=embeddings
)

prompt_template = PromptTemplate(
    template=SYSTEM_PROMPT_TEMPLATE, 
    input_variables=["context", "question"]
)

llm = DeepSeekLLM()

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template}
)

def preguntar(pregunta: str) -> Tuple[str, float]:
    """
    Ejecuta el flujo RAG completo: BÃºsqueda vectorial + GeneraciÃ³n.
    
    Realiza un cÃ¡lculo de precisiÃ³n ajustada para normalizar la distancia L2 
    de ChromaDB a un porcentaje de confianza (0.0 a 1.0).

    Args:
        pregunta (str): Consulta del usuario.

    Returns:
        Tuple[str, float]: Contiene (Respuesta generada, Nivel de confianza normalizado).
    """
    # 1. RecuperaciÃ³n con puntaje de distancia (Distance Score)
    # k=3 recupera los 3 fragmentos mÃ¡s cercanos semÃ¡nticamente
    docs_con_score = vectorstore.similarity_search_with_score(pregunta, k=3)
    
    scores_normalizados = []
    
    print(f"\nðŸ” Contexto recuperado para: '{pregunta}'")
    
    for doc, distance_score in docs_con_score:
        # ConversiÃ³n base: Distancia L2 -> Similitud (0 a ~0.5)
        similitud_base = 1 / (1 + distance_score)
        
        # CalibraciÃ³n heurÃ­stica: Escalamos el valor para reflejar mejor la 
        # percepciÃ³n humana de similitud en este dominio especÃ­fico.
        similitud_ajustada = min(similitud_base * SIMILARITY_SCALE_FACTOR, 1.0)
        
        scores_normalizados.append(similitud_ajustada)
        print(f"   - [Score: {similitud_ajustada:.2f}] {doc.page_content[:60]}...")
    
    # Promedio de confianza de los documentos recuperados
    precision_final = sum(scores_normalizados) / len(scores_normalizados) if scores_normalizados else 0.0

    # 2. GeneraciÃ³n de respuesta (LLM)
    resultado = qa_chain.invoke({"query": pregunta})
    texto_respuesta = resultado["result"]
    
    return texto_respuesta, precision_final

def agregar_datos(nuevos_textos: List[str]) -> None:
    """
    Ingesta nuevos documentos en la base vectorial ChromaDB.
    El guardado es automÃ¡tico en versiones recientes de langchain-chroma.
    """
    if nuevos_textos:
        vectorstore.add_texts(nuevos_textos)
        print(f"âœ… Se han vectorizado e indexado {len(nuevos_textos)} nuevos fragmentos.")

if __name__ == "__main__":
    print("Sistema RAG Inicializado (Modo CLI). Escribe 'salir' para terminar.")
    while True:
        user_input = input(">> ")
        if user_input.lower() in ["salir", "exit"]:
            break
        
        resp, conf = preguntar(user_input)
        print(f"Bot (Confianza {conf:.2f}): {resp}\n")